{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a28d8cf",
   "metadata": {},
   "source": [
    "This part is the detector part of control group.\n",
    "\n",
    "Basic method is using HoG+SVM to do a \"Insect or not\" classifier, find all possible areas that could be insects, output the location of these area for further classifiying. \n",
    "\n",
    "第一部分是对照组的检测器部分\n",
    "\n",
    "基础思路是结合HoG和SVM做一个是否是昆虫还是背景的二分类器，先将所有可能是昆虫的区域检出，将区域输出以进行进一步的昆虫类别分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91c78fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from sklearn.svm import LinearSVC\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdbffd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HoG_Detector:\n",
    "    def __init__(self):\n",
    "        self.hog_parameters ={\n",
    "            'pixels_per_cell':(8,8),\n",
    "            'cells_per_block':(2,2),\n",
    "            'orientations': 11\n",
    "        }\n",
    "\n",
    "    def filter_low_confidence_part(self,detections,min_confidence=1.0):\n",
    "        filtered_detections=[]\n",
    "        for detection in detections:\n",
    "            if detection['confidence'] >=min_confidence:\n",
    "                filtered_detections.append(detection)\n",
    "        return filtered_detections\n",
    "\n",
    "    def load_dataset_config(self,data_yaml_path):\n",
    "        with open(data_yaml_path,'r') as y: # Load yaml file 加载数据集的yaml配置文件\n",
    "            self.dataset_config=yaml.safe_load(y)\n",
    "        return self.dataset_config\n",
    "    \n",
    "    def get_positive_sample(self,image_dir,label_dir):\n",
    "        positive_sample=[]\n",
    "\n",
    "        for img_name in os.listdir(image_dir):\n",
    "            if not img_name.lower().endswith('.jpg'):\n",
    "                continue\n",
    "\n",
    "            img_path = os.path.join(image_dir,img_name)\n",
    "            label_path = os.path.join(label_dir,img_name.replace('.jpg','.txt'))\n",
    "\n",
    "            if not os.path.exists(label_path):\n",
    "                continue\n",
    "\n",
    "            image=cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "            h,w = image.shape[:2]\n",
    "\n",
    "            with open(label_path,'r') as l:\n",
    "                for line in l:\n",
    "                    details=line.strip().split()\n",
    "                    if len(details)!=5:\n",
    "                        continue\n",
    "\n",
    "                    class_id,x_center,y_center,width,height=map(float,details)\n",
    "                    \n",
    "                    x_center_abs= x_center*w # Transform label detail into pixel location 把label文件里的归一化坐标转换为像素坐标\n",
    "                    y_center_abs= y_center*h\n",
    "                    width_abs = width*w\n",
    "                    height_abs = height*h\n",
    "\n",
    "                    x1=int(x_center_abs-width_abs/2)\n",
    "                    x2=int(x_center_abs+width_abs/2)\n",
    "                    y1=int(y_center_abs-height_abs/2)\n",
    "                    y2=int(y_center_abs+height_abs/2)\n",
    "                    x1, y1 = max(0, x1), max(0, y1)\n",
    "                    x2, y2 = min(w, x2), min(h, y2)\n",
    "\n",
    "                    if x2 > x1 and y2 > y1:\n",
    "                        insect_patch = image[y1:y2, x1:x2]\n",
    "                        positive_sample.append(insect_patch)\n",
    "                        # Cut out positive sample area and add it to the list 剪下昆虫区域并存储\n",
    "    \n",
    "        return positive_sample\n",
    "\n",
    "    def get_negative_sample(self, image_dir,label_dir, samples_per_image=10):\n",
    "        negative_sample=[]\n",
    "\n",
    "        for img_name in os.listdir(image_dir):\n",
    "            if not img_name.lower().endswith('.jpg'):\n",
    "                continue\n",
    "\n",
    "            img_path = os.path.join(image_dir,img_name)\n",
    "            label_path = os.path.join(label_dir,img_name.replace('.jpg','.txt'))\n",
    "\n",
    "            if not os.path.exists(label_path):\n",
    "                continue\n",
    "\n",
    "            image=cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "            h,w = image.shape[:2]\n",
    "\n",
    "            avoid_areas=[]\n",
    "            if os.path.exists(label_path):\n",
    "                with open(label_path, 'r') as l:\n",
    "                    for line in l:\n",
    "                        parts = line.strip().split()\n",
    "                        if len(parts) == 5:\n",
    "                            class_id, x_center, y_center, width, height = map(float, parts)\n",
    "                            x_center_abs= x_center*w \n",
    "                            y_center_abs= y_center*h\n",
    "                            width_abs = width*w\n",
    "                            height_abs = height*h\n",
    "\n",
    "                            x1=int(x_center_abs-width_abs/2)\n",
    "                            x2=int(x_center_abs+width_abs/2)\n",
    "                            y1=int(y_center_abs-height_abs/2)\n",
    "                            y2=int(y_center_abs+height_abs/2)\n",
    "                            avoid_areas.append((x1, y1, x2, y2)) # Get positive area like former part 想先前一样画出昆虫区域待用\n",
    "            \n",
    "            for _ in range(samples_per_image):\n",
    "                size = np.random.randint(30,min(w,h)//2)\n",
    "                x1=np.random.randint(0,w-size)\n",
    "                x2=x1+size\n",
    "                y1=np.random.randint(0,h-size)\n",
    "                y2=y1+size\n",
    "\n",
    "            \n",
    "                overlap = False\n",
    "                for gt_x1, gt_y1, gt_x2, gt_y2 in avoid_areas:\n",
    "                    iou = self.calculate_iou((x1, y1, x2, y2), (gt_x1, gt_y1, gt_x2, gt_y2))\n",
    "                    if iou > 0.3:  # If overlapping is larger than 0.3, then we drop it 如果重叠度大于30%,认为是正样本区域,丢弃\n",
    "                        overlap = True\n",
    "                        break\n",
    "                \n",
    "                if not overlap:\n",
    "                    background_patch = image[y1:y2, x1:x2]\n",
    "                    negative_sample.append(background_patch)\n",
    "        \n",
    "        return negative_sample\n",
    "    \n",
    "    def calculate_iou(self,area1,area2):\n",
    "        x1_1, y1_1, x2_1, y2_1 = area1\n",
    "        x1_2, y1_2, x2_2, y2_2 = area2\n",
    "\n",
    "        # AND\n",
    "        a1 = max(x1_1,x1_2)\n",
    "        b1 = max(y1_1,y1_2)\n",
    "        a2 = max(x2_1,x2_2)\n",
    "        b2 = max(y2_1,y2_2)\n",
    "        inter_area=max(0,a2-a1)*max(0,b2-b1)\n",
    "\n",
    "        # OR\n",
    "        SofArea1= (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "        SofArea2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "        union_area=SofArea1+SofArea2-inter_area\n",
    "\n",
    "        if union_area>0:\n",
    "            return inter_area/union_area\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def extract_hog(self,image_patch):\n",
    "        # Extract HoG deatures 提取HoG特征\n",
    "        resized_p=cv2.resize(image_patch,(64,64))\n",
    "        # resized_p=image_patch\n",
    "        if len(resized_p.shape)==3: # If not gray 如果非灰度，转换为灰度\n",
    "            gray=cv2.cvtColor(resized_p,cv2.COLOR_BGR2GRAY)\n",
    "        else:\n",
    "            gray=resized_p\n",
    "        \n",
    "        hog_features=hog(gray,\n",
    "                         pixels_per_cell=self.hog_parameters['pixels_per_cell'],\n",
    "                         cells_per_block=self.hog_parameters['cells_per_block'],\n",
    "                         orientations=self.hog_parameters['orientations'],\n",
    "                         feature_vector=True)\n",
    "        return hog_features\n",
    "    \n",
    "    def prepare_train_data(self,data_yaml_path):\n",
    "        self.load_dataset_config(data_yaml_path)\n",
    "\n",
    "        train_image_dir = self.dataset_config['train'].replace('/images', '') + '/images'\n",
    "        train_label_dir = self.dataset_config['train'].replace('/images', '') + '/labels'\n",
    "\n",
    "        print(\"Preparing positive part 开始准备昆虫部分\")\n",
    "        positive_patch=self.get_positive_sample(train_image_dir,train_label_dir)\n",
    "        print(f\"Already found {len(positive_patch)} positive samples 找到了{len(positive_patch)}个正向样本\")\n",
    "\n",
    "        print(\"Preparing negative part 开始准备背景部分\")\n",
    "        negative_patch=self.get_negative_sample(train_image_dir,train_label_dir)\n",
    "        print(f\"Already found {len(negative_patch)} negative samples 找到了{len(negative_patch)}个反向样本\")\n",
    "\n",
    "        print(\"Extrating HoG features 提取Hog特征\")\n",
    "        positive_features=[self.extract_hog(i) for i in positive_patch]\n",
    "        negative_features=[self.extract_hog(i) for i in negative_patch]\n",
    "\n",
    "        X=positive_features+negative_features\n",
    "        y=[1]*len(positive_features)+[0]*len(negative_features) # 1 as Insect（昆虫） 0 as Background（背景）\n",
    "\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    def binary_classifier_train(self,data_yaml_path):\n",
    "        # Train the Insect & Background Classifier 训练昆虫/背景的二分类分类器\n",
    "        X,y=self.prepare_train_data(data_yaml_path)\n",
    "\n",
    "        print(f\"Trainning data:{X.shape},label:{y.shape}\")\n",
    "        print(f\"Positive samples:{np.sum(y==1)},negative samples={np.sum(y==0)}\")\n",
    "\n",
    "        self.svm = LinearSVC(C=1.0,class_weight='balanced',max_iter=10000)\n",
    "        self.svm.fit(X,y)\n",
    "\n",
    "        train_accuray= self.svm.score(X,y)\n",
    "        print(f\"Accuracy={train_accuray:.4f}\")\n",
    "\n",
    "        return train_accuray\n",
    "    \n",
    "    def sliding_window(self,image,window_size=(64,64),step_size=32,scale=1.0):\n",
    "        h,w = image.shape[:2]\n",
    "        window_w,window_h=window_size\n",
    "\n",
    "        if scale != 1.0:\n",
    "            new_w, new_h = int(w * scale), int(h * scale)\n",
    "            resized_image = cv2.resize(image, (new_w, new_h))\n",
    "        else:\n",
    "            resized_image = image\n",
    "            new_w, new_h = w, h\n",
    "\n",
    "        for y in range(0, new_h - window_h, step_size):\n",
    "            for x in range(0, new_w - window_w, step_size):\n",
    "                yield (x, y, resized_image[y:y+window_h, x:x+window_w], scale)\n",
    "    \n",
    "    def detect(self, image, confidence_threshold=0.4):\n",
    "        detections= []\n",
    "        h,w=image.shape[:2]\n",
    "\n",
    "        scales=[0.5,0.75,1.0,1.25,1.5,1.75,2.0]\n",
    "\n",
    "        for s in scales:\n",
    "            for x,y,window,current_scale in self.sliding_window(image,step_size=16,scale=s):\n",
    "                if window.size ==0:\n",
    "                    continue\n",
    "                try:\n",
    "                    features = self.extract_hog(window)\n",
    "                    confidence=self.svm.decision_function([features])[0]\n",
    "\n",
    "                    if confidence>confidence_threshold:\n",
    "                        orig_x=int(x/current_scale)\n",
    "                        orig_y=int(y/current_scale)\n",
    "                        orig_w=int(64/current_scale)\n",
    "                        orig_h=int(64/current_scale)\n",
    "\n",
    "                        # 计算原边界框的中心和尺寸\n",
    "                        center_x = orig_x + orig_w / 2\n",
    "                        center_y = orig_y + orig_h / 2\n",
    "                        new_w = orig_w * 3\n",
    "                        new_h = orig_h * 3\n",
    "                        # 计算放大后的边界框\n",
    "                        new_x1 = int(center_x - new_w / 2)\n",
    "                        new_y1 = int(center_y - new_h / 2)\n",
    "                        new_x2 = int(center_x + new_w / 2)\n",
    "                        new_y2 = int(center_y + new_h / 2)\n",
    "                        # 确保不超出图像边界\n",
    "                        new_x1 = max(0, new_x1)\n",
    "                        new_y1 = max(0, new_y1)\n",
    "                        new_x2 = min(w, new_x2)  # w是原图的宽度\n",
    "                        new_y2 = min(h, new_y2)  # h是原图的高度\n",
    "\n",
    "                        detections.append({   # RESULT 本部分检测结果\n",
    "                            'bbox': [new_x1, new_y1, new_x2, new_y2],\n",
    "                            'confidence': float(confidence)\n",
    "                        })\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        nms_d=self.non_max_suppression(detections)\n",
    "\n",
    "        filtered_d=self.filter_low_confidence_part(nms_d,min_confidence=1.3)\n",
    "\n",
    "        return filtered_d\n",
    "\n",
    "    \n",
    "    def non_max_suppression(self, detections, iou_threshold=0.4):\n",
    "        \"\"\"非极大值抑制去除重叠框\"\"\"\n",
    "        if len(detections) == 0:\n",
    "            return []\n",
    "        detections.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "        \n",
    "        keep = []\n",
    "        while detections:\n",
    "            best = detections.pop(0)\n",
    "            keep.append(best)\n",
    "            detections = [det for det in detections \n",
    "                         if self.calculate_iou(best['bbox'], det['bbox']) < iou_threshold]\n",
    "        \n",
    "        return keep\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17f35df",
   "metadata": {},
   "source": [
    "Then we use test dataset to do some test, see how the detector preform and save the detail of the result as csv\n",
    "\n",
    "接下来对Test集进行测试查看模型表现，并把结果保存为csv文件以供后续使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "009bcf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HogTesting:\n",
    "    def __init__(self,detector):\n",
    "        self.detector = detector\n",
    "        self.results =[]\n",
    "    \n",
    "    def detect_and_export(self,test_image_dir,test_label_dir,output_dir,confidence_threshold=0.5,Quick_detect=False,max_image=75,random_seed=47):\n",
    "        \n",
    "        os.makedirs(output_dir,exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_dir, 'candidate_patches'), exist_ok=True)\n",
    "\n",
    "        RESULT=[]\n",
    "        patch_info=[]\n",
    "\n",
    "        all_image_files = [f for f in os.listdir(test_image_dir) if f.lower().endswith('.jpg')]\n",
    "\n",
    "        if Quick_detect==True:\n",
    "            random.seed(random_seed)\n",
    "            image_files=random.sample(all_image_files,max_image)\n",
    "        else:\n",
    "            image_files=all_image_files\n",
    "\n",
    "        print(f\"Start processing {len(image_files)} images 开始处理测试图像\")\n",
    "\n",
    "        for image_index,image_name in enumerate(image_files):\n",
    "            image_path=os.path.join(test_image_dir,image_name)\n",
    "            image=cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "\n",
    "            Detection=self.detector.detect(image,confidence_threshold)\n",
    "            for detect_index,Detection in enumerate(Detection):\n",
    "                x1,y1,x2,y2=Detection['bbox']\n",
    "                confidence=Detection['confidence']\n",
    "\n",
    "                patch=image[y1:y2,x1:x2]\n",
    "                if patch.size==0:\n",
    "                    continue        \n",
    "                patch_filename=f\"{os.path.splitext(image_name)[0]}_patch_{detect_index}.jpg\"\n",
    "                patch_path=os.path.join(output_dir,'candidate_patches',patch_filename)\n",
    "                cv2.imwrite(patch_path,patch)\n",
    "\n",
    "                detection_info = {\n",
    "                    'original_image': image_name,\n",
    "                    'patch_filename': patch_filename,\n",
    "                    'bbox_x1': x1,\n",
    "                    'bbox_y1': y1,\n",
    "                    'bbox_x2': x2,\n",
    "                    'bbox_y2': y2,\n",
    "                    'confidence': confidence,\n",
    "                    'patch_path': patch_path\n",
    "                }\n",
    "\n",
    "                RESULT.append(detection_info)\n",
    "                patch_info.append(detection_info)\n",
    "        \n",
    "        df = pd.DataFrame(RESULT)\n",
    "        csv_path = os.path.join(output_dir, 'detection_results.csv')         # Save result as csv  保存检测结果为CSV\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        \n",
    "        json_path = os.path.join(output_dir, 'detection_results.json')        # Save result as json 保存为JSON格式\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(RESULT, f, indent=2)\n",
    "        \n",
    "        print(f\"Detection Finished,total {len(RESULT)} candidate areas 检测完成\")\n",
    "        print(f\"Result saved in {csv_path} CSV结果已保存\")\n",
    "        print(f\"Images saved in {os.path.join(output_dir, 'candidate_patches')} 图像已保存\")\n",
    "        \n",
    "        \n",
    "        return (RESULT,image_files) if Quick_detect==True else (RESULT,None)\n",
    "    \n",
    "    def load_ground_truth(self,label_dir,image_dir,image_list=None):\n",
    "        ground_truth = {}\n",
    "        label_files=[]\n",
    "        \n",
    "        if image_list==None:\n",
    "            label_files = [f for f in os.listdir(label_dir) if f.endswith('.txt')]\n",
    "        else:\n",
    "\n",
    "            for img_name in image_list:\n",
    "                label_f = img_name.replace('.jpg','.txt')\n",
    "                if os.path.exists(os.path.join(label_dir, label_f)):\n",
    "                    label_files.append(label_f)\n",
    "        \n",
    "        for label_file in label_files:\n",
    "            image_file = label_file.replace('.txt', '.jpg')\n",
    "            image_path = os.path.join(image_dir, image_file)\n",
    "            \n",
    "            if not os.path.exists(image_path):\n",
    "                continue\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "                \n",
    "            h, w = image.shape[:2]\n",
    "            \n",
    "            label_path = os.path.join(label_dir, label_file)\n",
    "            gt_boxes = []\n",
    "            \n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 5:\n",
    "                        continue\n",
    "                    \n",
    "                    class_id, x_center, y_center, width, height = map(float, parts)\n",
    "                    \n",
    "                    x_center_abs = x_center * w\n",
    "                    y_center_abs = y_center * h\n",
    "                    width_abs = width * w\n",
    "                    height_abs = height * h\n",
    "                    \n",
    "                    x1 = int(x_center_abs - width_abs/2)\n",
    "                    y1 = int(y_center_abs - height_abs/2)\n",
    "                    x2 = int(x_center_abs + width_abs/2)\n",
    "                    y2 = int(y_center_abs + height_abs/2)\n",
    "                    \n",
    "                    gt_boxes.append({\n",
    "                        'class_id': int(class_id),\n",
    "                        'bbox': [x1, y1, x2, y2]\n",
    "                    })\n",
    "            \n",
    "            ground_truth[image_file] = {\n",
    "                'image_size': (w, h),\n",
    "                'boxes': gt_boxes\n",
    "            }\n",
    "        \n",
    "        return ground_truth\n",
    "    \n",
    "    def check_preformance(self,detection_results,ground_truth,iou_threshold=0.5):\n",
    "        ''' This part is aim to evaluate the Hog+SVM preformance. We use the iou=0.5 as a threshold, only bigger than this threshold is a TP\n",
    "            这部分是detector的性能表现测试，阈值是iou=0.5，和test数据集label中提供的数值做对比，将iou大于这个数值的box视为区域检测正确\n",
    "        '''\n",
    "\n",
    "        detections_by_image = {}\n",
    "        for det in detection_results:\n",
    "            img_name = det['original_image']\n",
    "            if img_name not in detections_by_image:\n",
    "                detections_by_image[img_name] = []\n",
    "            detections_by_image[img_name].append(det)\n",
    "        \n",
    "        # Preformance 统计指标\n",
    "        total_gt_boxes = 0\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        false_negatives = 0\n",
    "        \n",
    "        detailed_results = []\n",
    "        \n",
    "        for img_name, gt_info in ground_truth.items():\n",
    "            if img_name not in detections_by_image:\n",
    "                false_negatives += len(gt_info['boxes'])\n",
    "                total_gt_boxes += len(gt_info['boxes'])\n",
    "                continue\n",
    "            \n",
    "            gt_boxes = gt_info['boxes']\n",
    "            detections = detections_by_image[img_name]\n",
    "            total_gt_boxes += len(gt_boxes)\n",
    "            gt_matched = [False] * len(gt_boxes)\n",
    "            det_matched = [False] * len(detections)\n",
    "            \n",
    "            for det_idx, detection in enumerate(detections):\n",
    "                det_bbox = [detection['bbox_x1'], detection['bbox_y1'], \n",
    "                           detection['bbox_x2'], detection['bbox_y2']]\n",
    "                best_iou = 0\n",
    "                best_gt_idx = -1\n",
    "                \n",
    "                for gt_idx, gt_box in enumerate(gt_boxes):\n",
    "                    if gt_matched[gt_idx]:\n",
    "                        continue\n",
    "                    \n",
    "                    iou = self.detector.calculate_iou(det_bbox, gt_box['bbox'])\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_gt_idx = gt_idx\n",
    "                \n",
    "                if best_iou >= iou_threshold and best_gt_idx != -1:\n",
    "                    # TP part 正确检测\n",
    "                    true_positives += 1\n",
    "                    gt_matched[best_gt_idx] = True\n",
    "                    det_matched[det_idx] = True\n",
    "                    \n",
    "                    detailed_results.append({\n",
    "                        'image': img_name,\n",
    "                        'detection_bbox': det_bbox,\n",
    "                        'gt_bbox': gt_boxes[best_gt_idx]['bbox'],\n",
    "                        'iou': best_iou,\n",
    "                        'confidence': detection['confidence'],\n",
    "                        'status': 'TP'\n",
    "                    })\n",
    "                else:\n",
    "                    # FP part 错误检测\n",
    "                    false_positives += 1\n",
    "                    detailed_results.append({\n",
    "                        'image': img_name,\n",
    "                        'detection_bbox': det_bbox,\n",
    "                        'gt_bbox': None,\n",
    "                        'iou': 0,\n",
    "                        'confidence': detection['confidence'],\n",
    "                        'status': 'FP'\n",
    "                    })\n",
    "            \n",
    "            # Miss 漏检\n",
    "            for gt_idx, matched in enumerate(gt_matched):\n",
    "                if not matched:\n",
    "                    false_negatives += 1\n",
    "                    detailed_results.append({\n",
    "                        'image': img_name,\n",
    "                        'detection_bbox': None,\n",
    "                        'gt_bbox': gt_boxes[gt_idx]['bbox'],\n",
    "                        'iou': 0,\n",
    "                        'confidence': 0,\n",
    "                        'status': 'FN'\n",
    "                    })\n",
    "        \n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / total_gt_boxes if total_gt_boxes > 0 else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        metrics = {\n",
    "            'true_positives': true_positives,\n",
    "            'false_positives': false_positives,\n",
    "            'false_negatives': false_negatives,\n",
    "            'total_ground_truth': total_gt_boxes,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1_score\n",
    "        }\n",
    "        \n",
    "        return metrics, detailed_results\n",
    "    \n",
    "    def generate_classification_dataset(self, detection_results, output_dir):\n",
    "        \"\"\" This part create the data structure used by CNN\n",
    "        生成CNN分类器训练/测试所需的数据集结构\n",
    "        \"\"\"\n",
    "        classification_dir = os.path.join(output_dir, 'classification_dataset')\n",
    "        os.makedirs(classification_dir, exist_ok=True)\n",
    "        classification_info = []\n",
    "        \n",
    "        for det in detection_results:\n",
    "            classification_info.append({\n",
    "                'patch_path': det['patch_path'],\n",
    "                'original_image': det['original_image'],\n",
    "                'bbox': f\"{det['bbox_x1']},{det['bbox_y1']},{det['bbox_x2']},{det['bbox_y2']}\",\n",
    "                'confidence': det['confidence']\n",
    "            })\n",
    "        \n",
    "        # Save csv file 保存csv文件分类数据集信息\n",
    "        classification_csv = os.path.join(classification_dir, 'classification_patches.csv')\n",
    "        pd.DataFrame(classification_info).to_csv(classification_csv, index=False)\n",
    "        \n",
    "        print(f\"CSV file created: {classification_csv}\")\n",
    "        print(f\"Including {len(classification_info)} candidate areas\")\n",
    "        \n",
    "        return classification_dir\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5933ee05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing positive part 开始准备昆虫部分\n",
      "Already found 15282 positive samples 找到了15282个正向样本\n",
      "Preparing negative part 开始准备背景部分\n",
      "Already found 21702 negative samples 找到了21702个反向样本\n",
      "Extrating HoG features 提取Hog特征\n",
      "Trainning data:(36984, 2156),label:(36984,)\n",
      "Positive samples:15282,negative samples=21702\n",
      "Accuracy=0.8706\n"
     ]
    }
   ],
   "source": [
    "detector = HoG_Detector()\n",
    "detector.binary_classifier_train('data.yaml')\n",
    "\n",
    "tester=HogTesting(detector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f6e6e6",
   "metadata": {},
   "source": [
    "We have a quick detect mode in our detect function. If set Quick detect as True, the function will randomly test some images.\n",
    "\n",
    "如果Quick Detect为True，测试时会从测试集中随机抽选一定量的样本，最大样本量和随机种子可指定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ecd7228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Detect and Export ===\n",
      "Start processing 546 images 开始处理测试图像\n",
      "Detection Finished,total 676 candidate areas 检测完成\n",
      "Result saved in detection_output\\detection_results.csv CSV结果已保存\n",
      "Images saved in detection_output\\candidate_patches 图像已保存\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Detect and Export ===\")\n",
    "\n",
    "test_image_dir = '../test/images'\n",
    "test_label_dir = '../test/labels' \n",
    "output_dir = 'detection_output'\n",
    "\n",
    "detection_results,processed_P = tester.detect_and_export(\n",
    "        test_image_dir, test_label_dir, output_dir,\n",
    "        confidence_threshold=0.3,\n",
    "        Quick_detect=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bc4fbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Load Ground Truth ===\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Load Ground Truth ===\")\n",
    "ground_truth = tester.load_ground_truth(test_label_dir, test_image_dir,processed_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4f0cbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 检测性能评估 ===\n",
      "Performance  检测性能指标:\n",
      "精确率 (Precision): 0.6612\n",
      "召回率 (Recall): 0.6488\n",
      "F1 F1分数: 0.6549\n",
      "TP 正确检测: 447\n",
      "FP 误检: 229\n",
      "Miss 漏检: 242\n",
      "Total real target 总真实目标: 689\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== 检测性能评估 ===\")\n",
    "metrics, detailed_results = tester.check_preformance(\n",
    "        detection_results, ground_truth, iou_threshold=0.5\n",
    "    )\n",
    "    \n",
    "print(f\"Performance  检测性能指标:\")\n",
    "print(f\"精确率 (Precision): {metrics['precision']:.4f}\")\n",
    "print(f\"召回率 (Recall): {metrics['recall']:.4f}\") \n",
    "print(f\"F1 F1分数: {metrics['f1_score']:.4f}\")\n",
    "print(f\"TP 正确检测: {metrics['true_positives']}\")\n",
    "print(f\"FP 误检: {metrics['false_positives']}\")\n",
    "print(f\"Miss 漏检: {metrics['false_negatives']}\")\n",
    "print(f\"Total real target 总真实目标: {metrics['total_ground_truth']}\")\n",
    "    \n",
    "evaluation_csv = os.path.join(output_dir, 'evaluation_metrics.csv')\n",
    "pd.DataFrame([metrics]).to_csv(evaluation_csv, index=False)\n",
    "    \n",
    "detailed_csv = os.path.join(output_dir, 'detailed_evaluation.csv')\n",
    "pd.DataFrame(detailed_results).to_csv(detailed_csv, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
