{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b783c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "\n",
    "class_n = 12\n",
    "\n",
    "data_folder = '../dataset' \n",
    "\n",
    "training_set_path = data_folder + '/train'\n",
    "validation_set_path = data_folder + '/valid'\n",
    "test_set_path = data_folder + '/test'\n",
    "#test_set_path = 'detection_output' \n",
    "\n",
    "retrain = True\n",
    "# Set to false to reuse existing parameters, and only run the test set for performance stats\n",
    "# Retraining is expensive so dont retrain unless metaparemeters are changed\n",
    "\n",
    "training_save_folder = ''\n",
    "\n",
    "#Meta parameters\n",
    "K = 500\n",
    "sift_Nfeatures = 500\n",
    "svm_regularisation = 10\n",
    "svm_gamma = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a146bf40",
   "metadata": {},
   "source": [
    "The \n",
    "1. Extract sift feature vectors from the images bounding boxes, package into BoVW\n",
    "2. use scikit svc classifier (kernal: tbd)\n",
    "\n",
    "Note that this program does not know if the input files are training set, validation set, or output from the hog detector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "498675f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load patches\n",
    "def loadPatchesLabels(path: str):\n",
    "    image_dir = path + '/images'\n",
    "    label_dir = path + '/labels'\n",
    "\n",
    "    patches = []\n",
    "    class_ids = []\n",
    "\n",
    "    for img_name in os.listdir(image_dir):\n",
    "        if not img_name.lower().endswith('.jpg'):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(image_dir,img_name)\n",
    "        label_path = os.path.join(label_dir,img_name.replace('.jpg','.txt'))\n",
    "\n",
    "        if not os.path.exists(label_path):\n",
    "            raise NameError('cannot find label at '+ label_path)\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            raise NameError('cannot find image at '+ img_path)\n",
    "        \n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        h,w = img.shape[:2]\n",
    "        \n",
    "        with open(label_path,'r') as l:\n",
    "            for line in l:\n",
    "                details=line.strip().split()\n",
    "                if len(details)!=5:\n",
    "                    raise ImportError\n",
    "                class_id,x_center,y_center,width,height=map(float,details)\n",
    "                    \n",
    "                x_center_abs= x_center*w # Transform label detail into pixel location\n",
    "                y_center_abs= y_center*h\n",
    "                width_abs = width*w\n",
    "                height_abs = height*h\n",
    "\n",
    "                x1=int(x_center_abs-width_abs/2)\n",
    "                x2=int(x_center_abs+width_abs/2)\n",
    "                y1=int(y_center_abs-height_abs/2)\n",
    "                y2=int(y_center_abs+height_abs/2)\n",
    "                x1, y1 = max(0, x1), max(0, y1)\n",
    "                x2, y2 = min(w, x2), min(h, y2)\n",
    "                insect_patch = img[y1:y2, x1:x2]\n",
    "                #print(f'patch added: {width_abs}x{height_abs}')\n",
    "\n",
    "                patches.append(insect_patch)\n",
    "                class_ids.append(class_id)\n",
    "    \n",
    "    return patches, class_ids\n",
    "\n",
    "valid_patches, valid_class_ids = loadPatchesLabels(validation_set_path)\n",
    "train_patches, train_class_ids = loadPatchesLabels(training_set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6bc071f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this builds the visual vocabulary for BoVW histogram\n",
    "def trainVisualWords(patches):\n",
    "    sift = cv2.SIFT_create(nfeatures = sift_Nfeatures)\n",
    "    descriptor_dictionary = []\n",
    "    for patch in patches:\n",
    "        keypoints, descriptors = sift.detectAndCompute(patch, None)\n",
    "        if descriptors is None:\n",
    "            print(\"Has a non-detection patch\")\n",
    "            continue\n",
    "        descriptor_dictionary.append(descriptors)\n",
    "    \n",
    "    descriptor_dictionary = np.vstack(descriptor_dictionary)\n",
    "    kmeans = KMeans(n_clusters=K, random_state=42, verbose=0)\n",
    "    kmeans.fit(descriptor_dictionary)\n",
    "\n",
    "    descriptor_vocabulary = kmeans.cluster_centers_\n",
    "    np.save(training_save_folder + \"vocabulary.npy\", descriptor_vocabulary)\n",
    "    return descriptor_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d205faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchToBoVW(patch, vocab):\n",
    "    sift = cv2.SIFT_create(nfeatures = sift_Nfeatures)\n",
    "    # Compute SIFT features for the new patch\n",
    "    keypoints, descriptors = sift.detectAndCompute(patch, None)\n",
    "    if descriptors is None:\n",
    "        bovw_vector = np.zeros(K)  # empty patch â†’ zero histogram\n",
    "    else:\n",
    "        distances = np.linalg.norm(descriptors[:, np.newaxis] - vocab, axis=2)\n",
    "        nearest_words = np.argmin(distances, axis=1)\n",
    "\n",
    "        # Build histogram (word frequency count)\n",
    "        bovw_vector, _ = np.histogram(nearest_words, bins=np.arange(K+1))\n",
    "        bovw_vector = normalize(bovw_vector.reshape(1, -1), norm='l2').flatten()\n",
    "    return bovw_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24555c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has a non-detection patch\n",
      "Has a non-detection patch\n",
      "Has a non-detection patch\n",
      "Has a non-detection patch\n",
      "Has a non-detection patch\n",
      "Has a non-detection patch\n",
      "Has a non-detection patch\n",
      "Has a non-detection patch\n",
      "Has a non-detection patch\n",
      "Has a non-detection patch\n",
      "Has a non-detection patch\n",
      "Has a non-detection patch\n",
      "Has a non-detection patch\n",
      "Has a non-detection patch\n",
      "Has a non-detection patch\n",
      "Has a non-detection patch\n",
      "Has a non-detection patch\n",
      "Has a non-detection patch\n"
     ]
    }
   ],
   "source": [
    "# Training process (BoVW)\n",
    "if retrain:\n",
    "    #The training pipeline\n",
    "    #Obtain vocabulary, this updates vocab file in save folder\n",
    "    vocab = trainVisualWords(train_patches)\n",
    "    # get BoVW for each patch\n",
    "    train_bovw_matrix = []\n",
    "    for patch in train_patches:\n",
    "        train_bovw_matrix.append(patchToBoVW(patch, vocab))\n",
    "else:\n",
    "    vocab = np.load(training_save_folder + 'vocabulary.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "38f0d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training process (SVM)\n",
    "if retrain:\n",
    "    clf = SVC(kernel='linear', C=svm_regularisation, gamma=svm_gamma, probability=True, class_weight='balanced')\n",
    "    clf.fit(train_bovw_matrix, train_class_ids)\n",
    "\n",
    "    joblib.dump(clf, training_save_folder + 'svm_model.bin')\n",
    "else:\n",
    "    clf = joblib.load(training_save_folder + 'svm_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08244241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[76., 22., 22., 17., 10., 11.,  5.,  3.,  4.,  2.,  6.,  0.],\n",
       "       [14., 57.,  8.,  9.,  1.,  3.,  5.,  1.,  4.,  2.,  5.,  1.],\n",
       "       [19., 12., 16.,  5.,  4., 14.,  9.,  3.,  5.,  4.,  5.,  4.],\n",
       "       [18., 15.,  6., 54., 13.,  4.,  5.,  5.,  8.,  4.,  4.,  3.],\n",
       "       [11.,  7.,  1.,  4., 21.,  3.,  3.,  3.,  9.,  6.,  1.,  3.],\n",
       "       [11.,  5., 19.,  7.,  6., 30.,  4.,  8.,  3.,  2.,  8.,  1.],\n",
       "       [ 7.,  4.,  9.,  7.,  8.,  5., 36.,  7.,  7.,  4.,  6.,  2.],\n",
       "       [ 1.,  4.,  2.,  4.,  6.,  3., 12., 59.,  5.,  2.,  2.,  1.],\n",
       "       [ 4.,  5.,  5., 13., 12.,  4.,  3.,  5., 19.,  9.,  5.,  7.],\n",
       "       [ 2.,  4., 12.,  5.,  5.,  5.,  4.,  4., 16., 39.,  2.,  9.],\n",
       "       [ 6., 14., 10.,  7.,  1., 12., 16.,  4.,  2.,  3., 57.,  0.],\n",
       "       [ 3.,  2.,  9.,  5.,  4.,  1.,  3.,  4.,  4.,  5.,  0., 65.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this pipeline on the validation set\n",
    "#initiate confusion matrix\n",
    "cm = np.zeros((class_n, class_n))\n",
    "for i in range(len(valid_patches)):\n",
    "    bovw = patchToBoVW(valid_patches[i], vocab)\n",
    "    predicted_class_id = int(clf.predict([bovw])[0])\n",
    "    true_class_id = int(valid_class_ids[i])\n",
    "    #print(f'Patch number {i}: true class {true_class_id} predicted class {predicted_class_id}')\n",
    "    cm[true_class_id][predicted_class_id] += 1\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f3525a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.39448173005219983)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistics on validation set\n",
    "total = 0\n",
    "correct = 0\n",
    "for i in range(12):\n",
    "    for j in range(12):\n",
    "        total += cm[i][j]\n",
    "        if (i == j):\n",
    "            correct += cm[i][j]\n",
    "accuracy = correct/total\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee4211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
